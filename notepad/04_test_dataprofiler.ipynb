{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/proyectos/dataprofiler/dataprofiler/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from common import get_sql_statement\n",
    "from aws_secrets_handler import AWSSecretsManager\n",
    "from db_manager import SQLProcessor, DatabaseConfig\n",
    "from aws_s3_handler import S3Manager\n",
    "from logger_config import get_logger\n",
    "\n",
    "# Configuración de logs\n",
    "logger = get_logger()\n",
    "\n",
    "APPMAINPATH = os.environ.get('APPMAINPATH')\n",
    "REGION_NAME = os.environ.get('REGION_NAME')\n",
    "BUCKET_NAME = os.environ.get('BUCKET_NAME')\n",
    "SECRET_NAME = os.environ.get('SECRET_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(sql_filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtiene un DataFrame a partir de una consulta SQL almacenada en un archivo.\n",
    "\n",
    "    Esta función:\n",
    "    - Obtiene credenciales de AWS Secrets Manager para conectarse a la base de datos.\n",
    "    - Configura la conexión a PostgreSQL.\n",
    "    - Carga la consulta SQL desde el archivo indicado.\n",
    "    - Ejecuta la consulta y devuelve los resultados en un DataFrame de pandas.\n",
    "\n",
    "    Args:\n",
    "        sql_filename (str): Nombre del archivo SQL dentro del directorio `/sql/`.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con los resultados de la consulta SQL.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si alguna de las variables de entorno necesarias no está definida.\n",
    "        FileNotFoundError: Si el archivo SQL no existe.\n",
    "        Exception: Si hay un error en la conexión a la base de datos o en la ejecución del SQL.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Obteniendo DataFrame desde la base de datos con la consulta '{sql_filename}'...\")\n",
    "    if not REGION_NAME or not SECRET_NAME or not APPMAINPATH:\n",
    "        logger.error(\"Las variables de entorno 'REGION_NAME', 'SECRET_NAME' o 'APPMAINPATH' no están definidas.\")\n",
    "        raise ValueError(\"Las variables de entorno 'REGION_NAME', 'SECRET_NAME' o 'APPMAINPATH' no están definidas.\")\n",
    "\n",
    "    logger.info(f\"Obteniendo DataFrame desde la base de datos con la consulta '{sql_filename}'...\")\n",
    "    sql_path = Path(APPMAINPATH) / \"sql\" / sql_filename\n",
    "\n",
    "    if not sql_path.exists():\n",
    "        logger.error(f\"El archivo SQL '{sql_path}' no existe.\")\n",
    "        raise FileNotFoundError(f\"El archivo SQL '{sql_path}' no existe.\")\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Obteniendo credenciales de AWS Secrets Manager...\")\n",
    "        with AWSSecretsManager(REGION_NAME) as sm:\n",
    "            secret = {\"db_type\": \"postgresql\", **sm.get_secret(SECRET_NAME)}\n",
    "\n",
    "        logger.info(\"Configurando conexión a la base de datos...\")\n",
    "        dbconfig = DatabaseConfig(**secret)\n",
    "        sqlprocessor = SQLProcessor(dbconfig)\n",
    "\n",
    "        logger.info(f\"Cargando consulta SQL desde el archivo '{sql_path}'...\")\n",
    "        sql = get_sql_statement(str(sql_path))\n",
    "\n",
    "        logger.info(\"Ejecutando consulta SQL...\")\n",
    "        return sqlprocessor.fetch_data(sql)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al obtener el DataFrame desde la base de datos: {e}\")\n",
    "        raise Exception(f\"Error al obtener el DataFrame desde la base de datos: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_profiling(df: pd.DataFrame, profile_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Genera un informe de perfilado de datos y lo guarda en formatos HTML, JSON y CSV.\n",
    "    Luego, sube estos archivos a un bucket de S3.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame a ser perfilado.\n",
    "        profile_name (str): Nombre del perfil para los archivos generados.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    actual_datetime: str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    html_filepath: str = f\"{APPMAINPATH}/profile_output/{actual_datetime}_{profile_name}.html\"\n",
    "    json_filepath: str = f\"{APPMAINPATH}/profile_output/{actual_datetime}_{profile_name}.json\"\n",
    "    csv_filepath: str = f\"{APPMAINPATH}/profile_output/{actual_datetime}_{profile_name}.csv\"\n",
    "    \n",
    "    profile = ProfileReport(df, title=f\"{actual_datetime}.{profile_name}\")\n",
    "    profile.to_file(html_filepath)\n",
    "    profile.to_file(json_filepath)\n",
    "\n",
    "    with open(json_filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    vars_to_omit: List[str] = [\"value_counts_without_nan\", \"value_counts_index_sorted\", \"histogram\"]\n",
    "    consolidated_data = {}\n",
    "    \n",
    "    for var in data[\"variables\"].keys():\n",
    "        consolidated_data[var] = {}\n",
    "        for key in data[\"variables\"][var].keys():\n",
    "            if key not in vars_to_omit:\n",
    "                consolidated_data[var][key] = data[\"variables\"][var][key]\n",
    "    \n",
    "    df_output = pd.DataFrame(consolidated_data).T.reset_index().rename(columns={\"index\": \"variable\"})\n",
    "    df_output.to_csv(csv_filepath, index=False)\n",
    "\n",
    "    s3_manager = S3Manager(REGION_NAME)\n",
    "    s3_manager.upload_file(html_filepath, BUCKET_NAME, f\"profiling/{actual_datetime}_{profile_name}.html\")\n",
    "    s3_manager.upload_file(json_filepath, BUCKET_NAME, f\"profiling/{actual_datetime}_{profile_name}.json\")\n",
    "    s3_manager.upload_file(csv_filepath, BUCKET_NAME, f\"profiling/{actual_datetime}_{profile_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-24 22:06:32.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_df\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mObteniendo DataFrame desde la base de datos con la consulta '01_dql_investments.sql'...\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_df\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mObteniendo DataFrame desde la base de datos con la consulta '01_dql_investments.sql'...\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mObteniendo credenciales de AWS Secrets Manager...\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_secrets_handler\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mCliente de Secrets Manager inicializado en la región us-east-2.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_secrets_handler\u001b[0m:\u001b[36mget_secret\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mObteniendo secreto: aws_postgres\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_secrets_handler\u001b[0m:\u001b[36m__exit__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mConexión al cliente de Secrets Manager cerrada.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_df\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mConfigurando conexión a la base de datos...\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdb_manager\u001b[0m:\u001b[36mconnect\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mConexión a postgresql establecida exitosamente.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_df\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mCargando consulta SQL desde el archivo '/home/ubuntu/proyectos/dataprofiler/sql/01_dql_investments.sql'...\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcommon\u001b[0m:\u001b[36mget_sql_statement\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mIntentando leer el archivo SQL desde: /home/ubuntu/proyectos/dataprofiler/sql/01_dql_investments.sql\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcommon\u001b[0m:\u001b[36mget_sql_statement\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mArchivo SQL leído correctamente desde: /home/ubuntu/proyectos/dataprofiler/sql/01_dql_investments.sql\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:32.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_df\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mEjecutando consulta SQL...\u001b[0m\n",
      "Summarize dataset: 100%|██████████| 25/25 [00:00<00:00, 26.32it/s, Completed]                     \n",
      "Generate report structure: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 201.25it/s]\n",
      "Render JSON: 100%|██████████| 1/1 [00:00<00:00, 47.74it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 945.30it/s]\n",
      "\u001b[32m2025-02-24 22:06:38.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_s3_handler\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mCliente S3 inicializado en la región us-east-2.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:38.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_s3_handler\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mSubiendo /home/ubuntu/proyectos/dataprofiler/profile_output/20250224_220634_market_prod.investments.html a s3://dwh-data-pr01/profiling/20250224_220634_market_prod.investments.html.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:40.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_s3_handler\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mArchivo /home/ubuntu/proyectos/dataprofiler/profile_output/20250224_220634_market_prod.investments.html subido exitosamente a profiling/20250224_220634_market_prod.investments.html en el bucket dwh-data-pr01.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:40.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_s3_handler\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mSubiendo /home/ubuntu/proyectos/dataprofiler/profile_output/20250224_220634_market_prod.investments.json a s3://dwh-data-pr01/profiling/20250224_220634_market_prod.investments.json.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:40.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_s3_handler\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mArchivo /home/ubuntu/proyectos/dataprofiler/profile_output/20250224_220634_market_prod.investments.json subido exitosamente a profiling/20250224_220634_market_prod.investments.json en el bucket dwh-data-pr01.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:40.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_s3_handler\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mSubiendo /home/ubuntu/proyectos/dataprofiler/profile_output/20250224_220634_market_prod.investments.csv a s3://dwh-data-pr01/profiling/20250224_220634_market_prod.investments.csv.\u001b[0m\n",
      "\u001b[32m2025-02-24 22:06:40.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36maws_s3_handler\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mArchivo /home/ubuntu/proyectos/dataprofiler/profile_output/20250224_220634_market_prod.investments.csv subido exitosamente a profiling/20250224_220634_market_prod.investments.csv en el bucket dwh-data-pr01.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "profile_name: str = \"market_prod.investments\"\n",
    "df = get_df(\"01_dql_investments.sql\")\n",
    "run_profiling(df, profile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprofiler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
